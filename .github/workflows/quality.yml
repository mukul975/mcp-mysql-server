name: Code Quality

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  lint-and-format:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black isort flake8 mypy pylint pre-commit
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi
    
    - name: Run Black (code formatter)
      run: |
        black --check --diff .
    
    - name: Run isort (import sorter)
      run: |
        isort --check-only --diff .
    
    - name: Run flake8 (style guide)
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Run mypy (type checking)
      run: |
        mypy . --ignore-missing-imports --no-strict-optional
    
    - name: Run pylint (code analysis)
      run: |
        pylint **/*.py --exit-zero

  test-coverage:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov coverage
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi
    
    - name: Run tests with coverage
      run: |
        pytest --cov=. --cov-report=xml --cov-report=html
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    - name: Upload coverage reports
      uses: actions/upload-artifact@v3
      with:
        name: coverage-report
        path: htmlcov/

  documentation:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install sphinx sphinx-rtd-theme pydoc-markdown
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Check docstrings
      run: |
        python -m pydoc -w .
    
    - name: Build documentation
      run: |
        if [ -d "docs" ]; then
          cd docs
          make html
        else
          echo "No docs directory found, skipping documentation build"
        fi

  dependency-check:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'
    
    - name: Install pip-audit
      run: |
        python -m pip install --upgrade pip
        pip install pip-audit
    
    - name: Run pip-audit
      run: |
        pip-audit --desc --format=json --output=audit-report.json || true
        pip-audit --desc || echo "Vulnerabilities found in dependencies"
    
    - name: Upload audit report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: dependency-audit
        path: audit-report.json

  performance-benchmark:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest-benchmark memory-profiler
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Run performance benchmarks
      run: |
        if [ -d "benchmarks" ]; then
          pytest benchmarks/ --benchmark-json=benchmark-results.json
        else
          echo "No benchmarks directory found, skipping performance tests"
        fi
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-results
        path: benchmark-results.json
